#include "OCLReader.h"

std::string Overclad::OCLAnalysis::OCLReader::GenerateCXXCode(std::string path)
{
    this->m_GenFeed = std::stringstream();
    m_OCLFeed.open(path);
    if (!m_OCLFeed.is_open())
    {
        std::cerr << "[ERR/LOG]: Error opening lexer specification file: " << path << std::endl;
        return this->m_GenFeed.str();
    }

    BeginReadProc();

    if (m_Lexemes.empty())
    {
        std::cerr << "[ERR/LOG]: No valid lexemes found." << std::endl;
        return this->m_GenFeed.str();
    }

    BuildFile();

    this->m_OCLFeed.close();

    return this->m_GenFeed.str();
}

Overclad::OCLAnalysis::LexemeEntry Overclad::OCLAnalysis::OCLReader::ParseLexemeEntry(std::string line)
{
    std::regex pattern(R"((\S+)\s*:\s*(.+))"); // TT: PAT
    std::smatch match;
    if (std::regex_match(line, match, pattern))
    {
        LexemeEntry entry;
        entry.TokenTypeName = match[1].str();
        entry.RegexPattern = match[2].str();
        return entry;
    }
    return {};
}

void Overclad::OCLAnalysis::OCLReader::BeginReadProc()
{
    std::string line = "";
    while (std::getline(m_OCLFeed, line))
    {
        if (line.empty() || line[0] == ';') continue;
        m_Lexemes.push_back(ParseLexemeEntry(line));
    }
}

void Overclad::OCLAnalysis::OCLReader::BuildFile()
{
    /* lexer.h */
    m_HGenFeed << "// lexer.h # Auto-Generated by Overclad //\n#pragma once\n";
    H_CREATE_INCLUDE("<iostream>");
    H_CREATE_INCLUDE("<regex>");
    H_CREATE_INCLUDE("<string>");
    H_CREATE_INCLUDE("<vector>");
    H_CREATE_INCLUDE("<unordered_map>");

    // create enum
    m_HGenFeed << "enum class TokenType\n{\n";
    CREATE_ENUM_ENTRY("UNDEF", 0);
    for (const auto& lexeme : m_Lexemes)
    {
        CREATE_ENUM_ENTRY(lexeme.TokenTypeName, -1);
    }
    CREATE_ENUM_ENTRY("_EOF", -1);
    H_CLOSE_SCOPE();

    // create Token struct
    m_HGenFeed << "struct Token\n{\n";
    m_HGenFeed << "\tTokenType Type;\n";
    m_HGenFeed << "\tstd::string Lexeme;\n";
    m_HGenFeed << "\tint line, col;\n";
    H_CLOSE_SCOPE();

    CREATE_FUNC_PROTO("void", "InitLexer", "const std::string& text");
    CREATE_FUNC_PROTO("std::vector<Token>&", "LexAll", "const std::string& text");

    /* lexer.cc */
    m_GenFeed << "// lexer.cc # Auto-Generated by Overclad //\n";
    m_GenFeed << "// NOTE: If you change the location of the include below, please reflect it in the include directive.\n";

    CREATE_INCLUDE("\"lexer.h\"");
    
    // generate pattern map
    m_GenFeed << "const std::unordered_map<TokenType, std::regex> tokenPatterns = {\n";
    for (const auto& lexeme : m_Lexemes)
    {
        m_GenFeed << "\t{ TokenType::" << lexeme.TokenTypeName << ", std::regex(" << lexeme.RegexPattern << ") },\n";
    }
    m_GenFeed << "};\n\n";

    m_HGenFeed << "const std::unordered_map<TokenType, std::string> tokenNames = {\n";
    for (const auto& lexeme : m_Lexemes)
    {
        m_HGenFeed << "\t{ TokenType::" << lexeme.TokenTypeName << ", \"" << lexeme.TokenTypeName << "\" },\n";
    }
    m_HGenFeed << "};\n\n";

    m_GenFeed << "std::vector<Token> tokens;\n";
    CREATE_FUNC_SIG("void", "InitLexer", "const std::string& text");
    CREATE_VAR("size_t", "currentPosition", 0);
    CREATE_VAR("int", "col", 0);
    CREATE_VAR("int", "line", 0);
    m_GenFeed << "\ttokens.clear();\n";
    m_GenFeed << "\tcurrentPosition = 0;\n";
    m_GenFeed << "\twhile (currentPosition < text.size()) {\n";
    m_GenFeed << "\t\tbool matched = false;\n\n";
    m_GenFeed << "\t\tfor (const auto& [type, pattern] : tokenPatterns) {\n";
    m_GenFeed << "\t\tstd::smatch match;\n";
    m_GenFeed << "\t\tstd::string substring = text.substr(currentPosition);\n";
    m_GenFeed << "\t\tif (std::regex_search(substring, match, pattern, std::regex_constants::match_continuous)) {\n";
    m_GenFeed << "\t\t\tif (type != TokenType::COMMENT && type != TokenType::WHITESPACE) { // remove the comment part if you do not have comments\n";
    m_GenFeed << "\t\t\t\ttokens.push_back({ type, match.str(0), line, col });\n\t\t\t}\n\n";
    m_GenFeed << "\t\t\t\tcurrentPosition += match.length(0);\n";
    m_GenFeed << "\t\t\t\tfor (size_t i = 0; i < match.length(0); ++i) {\n";
    m_GenFeed << "\t\t\t\t\tif (substring[i] == \'\\n\') {\n";
    m_GenFeed << "\t\t\t\t\t\tline++;\n";
    m_GenFeed << "\t\t\t\t\t\tcol = 1;\n";
    m_GenFeed << "\t\t\t\t\t} else {\n";
    m_GenFeed << "\t\t\t\t\t\tcol++;\n";
    m_GenFeed << "\t\t\t\t\t}\n";
    m_GenFeed << "\t\t\t\t}\n";
    m_GenFeed << "\t\t\t\tmatched = true;\n";
    m_GenFeed << "\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n";
    m_GenFeed << "\t\tif (!matched) {\n";
    m_GenFeed << "\t\t\tthrow std::runtime_error(\"Invalid token at position : \" + std::to_string(currentPosition));\n";
    m_GenFeed << "\t\t}\n";
    m_GenFeed << "\t}\n";
    CLOSE_SCOPE();

    CREATE_FUNC_SIG("std::vector<Token>&", "LexAll", "const std::string& text");
    m_GenFeed << "\ttry {\n";
    m_GenFeed << "\t\tInitLexer(text);\n";
    m_GenFeed << "\t\treturn tokens;\n";
    m_GenFeed << "\t}";
    m_GenFeed << " catch (const std::exception& e) {\n";
    m_GenFeed << "\t\tstd::cerr << \"Error: \" << e.what() << std::endl;\n";
    m_GenFeed << "\t\treturn tokens;\n";
    m_GenFeed << "\t}\n";
    CLOSE_SCOPE();
}
